# -*- coding: utf-8 -*-
"""Copy of Final Notebook RAW

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZdR3fc5hmvEfoZbnBlUdkEp9XhsG3v2

# Problem Statement

Dengan jumlah user yang semakin meningkat, banyak user
yang diacquired hanya untuk memenuhi jumlah user target dari
perusahaan Fashion Campus. Tim Marketing diberikan tugas
untuk membuat promo yang menarik untuk menarik perhatian
user. Namun, hal ini berdampak dengan banyaknya user yang
tidak organic, dimana mereka banyak yang tidak kembali lagi
ke platform untuk melakukan transaksi. Churn rate dari user
pun meningkat. **Team Data Science** diberikan tugas untuk menganalisa
kejadian ini dengan membuat churn prediction model.

# Data Preparation

## Import Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import datetime
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
from matplotlib.colors import ListedColormap
from sklearn import metrics
import warnings
import sys
if not sys.warnoptions:
    warnings.simplefilter("ignore")
np.random.seed(42)

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import matplotlib.font_manager
sns.set(rc={"axes.facecolor":"#FFF1E6",
            "figure.facecolor":"#EDDCD2"})
# pallet = ["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"]
# cmap = colors.ListedColormap(["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"])

pd.set_option('display.max_columns', None)

import warnings
warnings.filterwarnings("ignore")

"""## Load Dataset"""

customer_df = pd.read_csv('/content/drive/MyDrive/startupcampus/customer.csv')
product_df = pd.read_csv('/content/drive/MyDrive/startupcampus/product.csv', error_bad_lines=False)
transactions_df = pd.read_csv('/content/drive/MyDrive/startupcampus/transactions.csv')
# click_stream_df = pd.read_csv('/content/drive/MyDrive/startupcampus/new_click_stream.csv')

"""## Tabel Customer"""

customer_df.head()

customer_df.info()

"""## Tabel Product"""

product_df.head()

product_df.info()

"""## Tabel Transcations"""

transactions_df.head()

transactions_df.info()

"""### Handling Metadata"""

import ast

tmp = transactions_df.iloc[:,4].astype('str')
tmp = tmp.map(lambda x:ast.literal_eval(x))
tmp = tmp.to_frame()

transactions_df['product_metadata'] = tmp['product_metadata']
transactions_df = transactions_df.explode('product_metadata')

transactions_df.reset_index()

tmp = pd.json_normalize(transactions_df.product_metadata)

transactions_df = pd.concat([transactions_df.reset_index(), tmp], join='outer',axis=1)

transactions_df.drop(transactions_df.columns[[0,5]],axis=1, inplace=True)
transactions_df.head()

transactions_df.info()

"""# Data Cleansing

## Combining Dataset
"""

product_df.rename(columns={'id':'product_id'}, inplace=True)

product_df.drop('gender', axis=1, inplace=True)

dataset1 = pd.merge(customer_df, transactions_df, how='right', on='customer_id')

dataset = pd.merge(dataset1, product_df, how='left', on='product_id')

dataset.head()

dataset.info()

dataset.isnull().sum()

"""## Handling Missing Values"""

filled_dataset = dataset.copy()

filled_dataset['promo_code'] = filled_dataset['promo_code'].fillna('None')

null_categorical = filled_dataset[['masterCategory',
                                  'subCategory',
                                  'articleType', 
                                  'baseColour',
                                  'season',
                                  'usage',
                                  'productDisplayName']]

filled_dataset = filled_dataset.fillna(null_categorical.mode().iloc[0])

filled_dataset = filled_dataset.fillna(filled_dataset['year'].mode()[0])

filled_dataset.head(3)

filled_dataset.info()

"""## Reformatting Data Types"""

formatted_dataset = filled_dataset.copy()

object_columns = ['customer_id',
                  'product_id',
                  'home_location_lat',
                  'home_location_long',
                  'shipment_location_lat',
                  'shipment_location_long']

formatted_dataset[object_columns] = formatted_dataset[object_columns].astype(str)

formatted_dataset['year'] = formatted_dataset['year'].astype(int)

from datetime import datetime, date

date_columns = ['birthdate',
                'first_join_date',
                'created_at',
                'shipment_date_limit']

formatted_dataset[date_columns] = formatted_dataset[date_columns].apply(pd.to_datetime, errors='coerce')

date_columns = ['created_at', 'shipment_date_limit']

formatted_dataset['created_at'] = formatted_dataset['created_at'].dt.strftime('%Y-%m-%d')
formatted_dataset['shipment_date_limit'] = formatted_dataset['shipment_date_limit'].dt.strftime('%Y-%m-%d')

formatted_dataset[date_columns] = formatted_dataset[date_columns].apply(pd.to_datetime, errors='coerce')

"""## Renaming Columns"""

renamed_dataset = formatted_dataset.copy()

renamed_dataset.rename(columns = {'customer_id':'Customer ID',
                                  'first_name':'First Name',
                                  'last_name':'Last Name',	
                                  'username':'Username',	
                                  'email':'Email',
                                  'gender':'Gender',	
                                  'birthdate':'Birthday',	
                                  'device_type':'Device Type',	
                                  'device_id':'Device ID',	
                                  'device_version':'Device Version',	
                                  'home_location_lat':'Home Location Latitude',	
                                  'home_location_long':'Home Location Longitude',	
                                  'home_location':'Home Location',
                                  'home_country':'Home Country',	
                                  'first_join_date':'First Join Date',	
                                  'created_at':'Created At',
                                  'booking_id':'Booking ID',	
                                  'session_id':'Session ID',	
                                  'payment_method':'Payment Method',	
                                  'payment_status':'Payment Status',	
                                  'promo_amount':'Promo Amount',	
                                  'promo_code':'Promo Code',	
                                  'shipment_fee':'Shipment Fee',	
                                  'shipment_date_limit':'Shipment Date Limit',	
                                  'shipment_location_lat':'Shipment Location Latitude',	
                                  'shipment_location_long':'Shipment Location Longitude',	
                                  'total_amount':'Total Amount',	
                                  'product_id':'Product ID',	
                                  'quantity':'Quantity',	
                                  'item_price':'Item Price',	
                                  'masterCategory':'Master Category',	
                                  'subCategory':'Sub Category',	
                                  'articleType':'Article Type',	
                                  'baseColour':'Base Colour',	
                                  'season':'Season',	
                                  'year':'Year',	
                                  'usage':'Usage',	
                                  'productDisplayName':'Product Display Name'}, inplace = True)

cleaned_dataset = renamed_dataset.copy()

cleaned_dataset.head(3)

# cleaned_dataset.to_csv('dataset_full.csv')

"""# Exploratory Data Analysis

## Univariate

### Analisis Deskriptif
"""

cleaned_dataset.info()

for col in cleaned_dataset:
    print(f"\033[1m{col} \n{20 * '-'}\033[0m")
    print(cleaned_dataset[col].value_counts(), '\n')

cleaned_dataset.describe()

"""> * Rata-rata `Promo Amount` (potongan harga) adalah sekitar 2000, paling kecil adalah 0 atau tidak potongan harga sama sekali dan paling besar adalah  sekitar 2400. Sepertinya potongan harganya tidak terlalu membantu ðŸ˜
* Rata-rata `Shipment Fee` (ongkos kirim) adalah sekitar 9000, paling murah 0 (gratis ongkir ðŸ˜†) dan paling mahal adalah 50.000, apakah lokasi yang jauh ataukah ukuran barang pesanannya yang besar (?)
* Rata-rata `Total Amount` (total belanja) yang dikeluarkan adalah 1.181.000, paling sedikit adalah 10.000, paling banyak adalah 23.504.000
* Rata-rata dan jumlah paling sedikit `Quantity` (jumlah pesanan) adalah 1, dan paling banyak adalah 48 buah
* Rata rata `Item Price` (harga item) adalah sekitar 250.000, paling murah adalah 5600, dan paling mahal adalah 1.200.000
"""

cleaned_dataset.describe(include=object)

"""> * Mayoritas Pelanggan adalah sebagai berikut:
 - Berjenis Kelamin Perempuan ðŸ‘©ðŸ»
 - Pengguna Android 
 - Berdomisili di Jakarta
 - Metode Pembayaran Kartu Kredit
 - Tidak pernah menggunakan Kode Promo
 - Produk yang dibeli adalah pakaian

### Analisis Setiap Kolom Kategorikal dengan Visualisasi
"""

categorical_columns = ['Gender',
                       'Device Type',
                       'Home Location',
                       'Payment Method',
                       'Payment Status',
                       'Promo Code',
                       'Master Category',
                       'Season',
                       'Usage']

plt.figure(figsize = (40,30))
for i in range(0, len(categorical_columns)):
    plt.subplot(3, 3, i+1)
    ax = sns.countplot(x=cleaned_dataset[categorical_columns[i]], palette='magma', orient='h')
    # ax.bar_label(ax.containers[0])
    ax.tick_params(axis='both', which='major', pad=10)
    plt.tight_layout()
    plt.xticks(rotation=90,fontsize=20)
    plt.yticks(fontsize=20)
    plt.ylabel(ylabel='Count',fontsize=35)
    plt.xlabel(xlabel=categorical_columns[i],fontsize=35)

categorical_columns = ['Gender',
                       'Device Type',
                       'Home Location',
                       'Payment Method',
                       'Payment Status',
                       'Promo Code',
                       'Master Category',
                       'Season',
                       'Usage']

plt.figure(figsize = (25,50))
for i in range(0, len(categorical_columns)):
    plt.subplot(5, 2, i+1)
    ax = sns.countplot(x=cleaned_dataset[categorical_columns[i]], palette='magma', orient='h')
    # ax.bar_label(ax.containers[0])
    ax.tick_params(axis='both', which='major', pad=10)
    plt.tight_layout()
    plt.xticks(rotation=90,fontsize=20)
    plt.yticks(fontsize=20)
    plt.ylabel(ylabel='Count',fontsize=35)
    plt.xlabel(xlabel=categorical_columns[i],fontsize=35)

"""> Insight:
* Perempuan lebih banyak melakukan transaksi dibandingkan Laki-Laki
* Pengguna Android lebih banyak dibandingkan iOS
* Domisili pelanggan peringkat satu diraih oleh Jakarta, disusul oleh Jawa Barat, Jawa Tengah, Jawa Timur, dan Yogyakarta. Ternyata peminat tertinggi berada di pulau Jawa (Djawa adalah Koentji ðŸ”‘? )
* Metode Pembayaran paling banyak menggunakan Kartu Kredit
* Pelanggan lebih banyak melakukan transaksi tanpa menggunakan Kode Promo. Apakah karena promo nya tidak terlalu berpengaruh seperti yang sudah dibahas tadi(?)....hmmm ðŸ¤”
* Produk yang paling diminati adalah Pakaian, disusul Aksesoris, dan Alas Kaki.
* Transaksi paling banyak dilakukan pada musim Panas, disusul musim Gugur, musim Dingin dan musim Semi. Tapi bukannya Indonesia hanya punya 2 musim ya, ataukah ini hanya menggunakan acuan musim global(?) ðŸŒŽ
* Produk yang dibeli pelanggan paling popular adalah untuk keperluan santai ðŸ

## Bivariate

### Distribusi Penjualan
"""

df_date = cleaned_dataset[['Customer ID','Created At']].groupby(by='Created At').count().reset_index()
df_date

data_train = cleaned_dataset.set_index('Created At')
data_by_month = data_train.resample('A').count()
time = data_by_month.index.values

time = data_by_month.index.values
test = data_by_month['Customer ID'].values

plt.figure(figsize=(20,5))
plt.plot(time, test)
plt.title('Grafik Total Penjualan per Bulan', fontsize = 30)
plt.ylabel('Value', fontsize = 20)
plt.xlabel('Year', fontsize = 20)
plt.grid(color='grey')

df_year = cleaned_dataset[['Customer ID','Created At']].copy()
df_year['year'] = df_year['Created At'].dt.year  # extracting year
df_year.head()

df_year = df_year[['Customer ID','year']].groupby(by='year').count().reset_index()
df_year

sns.catplot(x='year',y='Customer ID',data=df_year,kind='bar',aspect=3, palette='magma')
plt.show()

"""## Multivariate

### Korelasi Antarkolom Numerikal
"""

cleaned_dataset['Year'] = cleaned_dataset['Year'].astype(str)

corrmat= cleaned_dataset.corr()
plt.figure(figsize=(10,10))  
sns.heatmap(corrmat,annot=True, linewidth=1, cmap='magma_r')

"""> Insight
* Tidak terdapat kolom yang memiliki korelasi kuat satu dengan lainnya

### Total Belanja per Tahun berdasarkan Gender
"""

cleaned_dataset2 = cleaned_dataset
cleaned_dataset2['Created At'] = cleaned_dataset2['Created At'].dt.strftime('%Y')

cleaned_dataset2['Created At'] = cleaned_dataset2['Created At'].astype(int)

plt.figure(figsize=(15,7))
sns.lineplot(
    x='Created At', 
    y='Total Amount',
    data=cleaned_dataset2,
    palette='magma',
    hue='Gender')
plt.title('Total Amount per Year by Gender', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Year', fontsize=15)
plt.ylabel('Total Amount', fontsize=15)

"""> Insight
* Total belanja pelanggan Perempuan ðŸ‘©ðŸ» lebih mendominasi, terutama pada tahun 2016 - 2018, serta pada tahun 2020 & 2021
* Untuk Laki-Laki ðŸ‘¨ðŸ» Total Belanja lebih unggul sedikit pada tahun 2019 & 2022

### Jumlah Pesanan tiap Kategori Produk berdasarkan Jenis Kelamin
"""

plt.figure(figsize=(15,7))
sns.barplot(
    x='Master Category', 
    y='Quantity', 
    data=cleaned_dataset,
    palette='magma',
    hue='Gender')
plt.title('Quantity of Master Category by Gender', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Master Category', fontsize=15)
plt.ylabel('Quantity', fontsize=15)

"""> Insight
* Jumlah pembelian suatu produk berdasarkan kategorinya tidak dipengaruhi oleh jenis kelamin Pelanggan, pembelian produk berdasarkan kategori relatif seimbang antara Perempuan dan Laki-Laki, kecuali pada pada kategori yang memiliki transaksi pembelian sedikit pada grafik sebelumnya (`Free Items`, `Sporting Goods`, `Home`)

### Hubungan antara Ongkos Kirim dengan Kode Promo berdasarkan Jenis Kelamin
"""

plt.figure(figsize=(15,7))
sns.catplot(
    x='Promo Code', 
    y='Promo Amount',
    data=cleaned_dataset,
    palette='magma',
    split=True,
    height=6.5,
    aspect=2,
    kind='violin',
    hue='Gender')
plt.title('Shipment Fee with Promo Code by Gender', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Promo Code', fontsize=15)
plt.ylabel('Shipment Fee', fontsize=15)

"""> Insight
* Terdapat 2 buah Kode Promo yang masih memilki rata-rata biaya ongkos kirim 10.000, atau 2x lipatnya dari Kode Promo yang lain yaitu 5000. Penggunaan Kode Promo juga relatif seimbang antara Perempuan dan Laki-Laki

### Distribusi antara Harga Produk dengan Jumlah Pesanan berdasarkan Kategori Produk
"""

plt.figure(figsize=(15,7))
sns.scatterplot(
    x='Quantity',
    y='Item Price',
    data=cleaned_dataset,
    palette='magma',
    hue='Master Category')
plt.title('Plotting Distribution between Item Price and Quantity by Master Category', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Quantity', fontsize=15)
plt.ylabel('Item Price', fontsize=15)

"""> Insight
* Terlihat bahwa semakin mahal harga produk yang dibeli semakin sedikit pula jumlah yang di pesan oleh pelanggan, begitu pun sebaliknya semakin banyak jumlah yang di pesan oleh pelanggan semakin murah pula harga produk nya (Korelasi Negatif)

### Hubungan Ongkos Kirim dan Kategori Produk berdasarkan Musim
"""

plt.figure(figsize=(15,7))
sns.boxplot(
    x='Master Category', 
    y='Shipment Fee',
    data=cleaned_dataset,
    palette='magma',
    hue='Season')
plt.title('Shipment Fee and Master Category by Season', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Master Category', fontsize=15)
plt.ylabel('Shipment Fee', fontsize=15)

"""> Insight
* Biaya ongkos kirim pada tiap musim relatif sama yaitu memiliki Interquartile antara 0 - 25.000, walaupun terdapat juga beberapa biaya ongkos kirim sebesar 50.000 (Outlier). Terdapat juga beberapa kategori produk yang tidak dibeli pada musim-musim tertentu, contohnya tidak ada pembelian kategori produk Personal Care pada musim Gugur, atau kategori produk Home hanya dibeli pada musim Gugur saja.

# Feature Engineering

## Feature Construction

### Time Benchmark
"""

fix_dataset = cleaned_dataset.copy()

from datetime import datetime, date
current_time = date.today()
current_time = pd.to_datetime(current_time)
current_time

"""### Age"""

fix_dataset['Age'] = current_time - fix_dataset['Birthday']
fix_dataset['Age'] = round(fix_dataset['Age'] / np.timedelta64(1, "Y")).astype(int)

"""### Last Purchased"""

last_purchased = fix_dataset.groupby('Customer ID')['Created At'].max().reset_index()
last_purchased = last_purchased.rename({'Created At':'Last Purchased'},axis = 1)
fix_dataset = fix_dataset.merge(last_purchased, on = 'Customer ID')

"""### Duration"""

import datetime as dt
fix_dataset['Duration'] = (fix_dataset['Last Purchased'] - fix_dataset['First Join Date']).dt.days

"""### Recency"""

recency = fix_dataset['Created At'].max()
fix_dataset['Recency'] = (recency - fix_dataset['Created At']).dt.days

"""### Frequency"""

customer_repurchase = fix_dataset[['Created At','Customer ID']].groupby('Customer ID').count()
customer_repurchase.rename(columns={'Created At':'Frequency'}, inplace=True)
fix_dataset = fix_dataset.merge(customer_repurchase, how='left', on = 'Customer ID')

"""### Monetary"""

total_purchase = fix_dataset.groupby('Customer ID')['Total Amount'].sum().reset_index()
total_purchase = total_purchase.rename({'Total Amount':'Monetary'},axis = 1)
fix_dataset = fix_dataset.merge(total_purchase, on = 'Customer ID')

"""### Total Item"""

total_quantity = fix_dataset.groupby('Customer ID')['Quantity'].sum().reset_index()
total_quantity = total_quantity.rename({'Quantity':'Total Item'},axis = 1)
fix_dataset = fix_dataset.merge(total_quantity, on = 'Customer ID')

"""### Total Promo Amount"""

promo_amount = fix_dataset[['Promo Amount','Customer ID']].groupby('Customer ID').sum()
promo_amount.rename(columns={'Promo Amount':'Total Promo Amount'}, inplace=True)
fix_dataset = fix_dataset.merge(promo_amount, on = 'Customer ID')

"""### Total Promo Code Used"""

customer_promo = fix_dataset[['Promo Code','Customer ID']].groupby('Customer ID').count()
customer_promo.rename(columns={'Promo Code':'Total Promo Code Used'}, inplace=True)
fix_dataset = fix_dataset.merge(customer_promo, on = 'Customer ID')

"""### Main Payment Method"""

main_payment = fix_dataset[['Customer ID', 'Payment Method']].groupby('Customer ID').agg(pd.Series.mode)
main_payment.rename(columns={'Payment Method':'Main Payment Method'}, inplace=True)
fix_dataset = fix_dataset.merge(main_payment, how='left', on = 'Customer ID')

"""### Estimated Delivery"""

fix_dataset['Estimated Delivery'] = fix_dataset['Shipment Date Limit'] - fix_dataset['Created At']
fix_dataset['Estimated Delivery'] = round(fix_dataset['Estimated Delivery'] / np.timedelta64(1, "D")).astype(int)

"""### Max Shipping"""

max_shipping = fix_dataset.groupby('Customer ID')['Estimated Delivery'].max().reset_index()
max_shipping = max_shipping.rename({'Estimated Delivery':'Max Shipping'},axis = 1)
fix_dataset = fix_dataset.merge(max_shipping, how='left', on = 'Customer ID')

"""### Main Location"""

def main_location(x):
    if (x == 'Jakarta Raya' or
        x == 'Jawa Barat' or
        x == 'Jawa Tengah' or
        x == 'Jawa Timur' or
        x == 'Yogyakarta' or
        x == 'Banten'):
      return 'Pulau Jawa'
    else:
      return 'Luar Pulau Jawa'

fix_dataset['Main Location'] = fix_dataset['Home Location'].apply(lambda x: main_location(x))

"""### **IS CHURN**"""

fix_dataset['Is Churn'] = fix_dataset['Recency'].apply(lambda x: True if x >= 365 else False)

fix_dataset['Is Churn'].value_counts()

fix_dataset.head(3)

"""### Reduce Dataset"""

reduced_dataset = fix_dataset.drop_duplicates(subset='Customer ID', keep='last')
reduced_dataset = reduced_dataset.reset_index(drop=True)

reduced_dataset = reduced_dataset.drop('Main Payment Method', axis=1)

# reduced_dataset.to_csv('reduced_dataset.csv', index=False)

reduced_dataset.head(3)

reduced_dataset.info()

for col in reduced_dataset:
    print(f"\033[1m{col} \n{20 * '-'}\033[0m")
    print(reduced_dataset[col].value_counts(), '\n')

"""### **CHURN Analysis**

#### Churn Persentage
"""

reduced_dataset['Is Churn'].value_counts()

fig, ax = plt.subplots()
labels = ['False','True']
churn = reduced_dataset['Is Churn'].value_counts()
ax.pie(churn, labels=labels, autopct='%.0f%%')
plt.title('Percetage of Customers Churn', fontsize=20)
plt.show()

"""#### Categorical"""

cat_cols = ['Gender',
            'Device Type',
            'Payment Method',
            'Promo Code',
            'Master Category',
            'Season',
            'Year',
            'Usage',
            'Main Location']

plt.figure(figsize = (40,30))
for i in range(0, len(cat_cols)):
    plt.subplot(3, 3, i+1)
    ax = sns.countplot(data=reduced_dataset, x=cat_cols[i], palette='magma', orient='h', hue='Is Churn')
    ax.tick_params(axis='both', which='major', pad=10)
    plt.tight_layout()
    plt.xticks(rotation=90,fontsize=20)
    plt.yticks(fontsize=20)
    plt.ylabel(ylabel='Count',fontsize=35)
    plt.xlabel(xlabel=cat_cols[i],fontsize=35)

plt.figure(figsize = (25,50))
for i in range(0, len(cat_cols)):
    plt.subplot(5, 2, i+1)
    ax = sns.countplot(data=reduced_dataset, x=cat_cols[i], palette='magma', orient='h', hue='Is Churn')
    ax.tick_params(axis='both', which='major', pad=10)
    plt.tight_layout()
    plt.xticks(rotation=90,fontsize=20)
    plt.yticks(fontsize=20)
    plt.ylabel(ylabel='Count',fontsize=35)
    plt.xlabel(xlabel=cat_cols[i],fontsize=35)

"""> Insight
* Bila diperhatikan dengan baik, hampir semua jumlah antara Churn 'True' dengan Churn 'False' memiliki perbandingan yang sama pada tiap sub kolom yaitu 1:3 atau Churn 'True' sekitar 30% nya dari Churn 'False'
 - Sebagai contoh pada Kolom `Gender` bahwa total Churn 'False' pada sub Kolom Male sekitar 15.000 sedangkan total Churn 'True' nya sekitar 5000, bisa dilihat bahwa perbandingan antara 'True' : 'Flase' adalah 1:3
* Tinggi atau rendahnya nilai Churn pada sub Kolom kali ini juga dipengaruhi oleh tinggi atau rendahnya transaksi pada grafik sebelumnya (Analisis Setiap Kolom Kategorikal dengan Visualisasi)
 - Sebagai contoh nilai Churn paling tinggi pada kolom `Usage` adalah sub kolom 'Casual', dimana 'Casual' sendiri pada grafik sebelumnya merupakan sub kolom yang paling tinggi nilai transaksinya

#### Numerical
"""

num_cols = ['Promo Amount',
            'Shipment Fee',
            'Total Amount',
            'Quantity',
            'Item Price',
            'Age',
            'Duration',
            'Recency',
            'Frequency',
            'Monetary',
            'Total Item',
            'Total Promo Amount',
            'Total Promo Code Used',
            'Estimated Delivery',
            'Max Shipping',
            'Is Churn']

sns.pairplot(reduced_dataset[num_cols], hue='Is Churn' , palette = 'magma')
plt.show()

"""> Insight
* Berdasarkan grafik diatas, pada kolom `Recency` terlihat jelas batas antara Churn `True` dan Churn `False`. Semakin tinggi angka `Recency`, maka semakin besar pula bahwa pelanggan itu dinyatakan Churn. Hal ini karena kolom Churn sendiri merupakan hasil Feature Engineering dari kolom `Recency`
* Churn `True` juga terlihat jelas pada kolom `Frequency`, `Monetary`, `Total Item`, `Total Promo Amount`, dan `Total Promo Code Used`. Semakin rendah nilai mereka semakin banyak didapatkan Churn `True`
* Terdapat beberapa kolom yang memiliki Korelasi Positif antara `Frequency`, `Monetary`, `Total Item`, dan `Total Promo Amount`

## Feature Selection
"""

reduced_dataset = pd.read_csv('/content/drive/MyDrive/startupcampus/reduced_dataset.csv')

reduced_dataset.info()

reduced_dataset.head(3)

reduced_dataset['Customer ID'] = reduced_dataset['Customer ID'].astype(str)

"""### Kolom Numerikal untuk Clustering"""

num_cols = reduced_dataset[['Customer ID',
                            'Promo Amount',
                            'Shipment Fee',
                            'Total Amount',
                            'Quantity',
                            'Item Price',
                            'Age',
                            'Duration',
                            'Recency',
                            'Frequency',
                            'Monetary',
                            'Total Item',
                            'Total Promo Amount',
                            'Total Promo Code Used',
                            'Estimated Delivery',
                            'Max Shipping']]

"""### Kolom Kategorikal untuk Clustering"""

cat_cols = reduced_dataset[['Customer ID',
                            'Gender',
                            'Device Type',
                            'Payment Method',
                            'Master Category',
                            'Season',
                            'Usage',
                            'Main Location',
                            'Is Churn']]

corrmat = num_cols.corr()
plt.figure(figsize=(16,16))  
sns.heatmap(corrmat,annot=True, linewidth=1, cmap='magma_r', fmt='.2f')

"""> Insight
* Beberapa kolom memeiliki korelasi yang sangat kuat yaitu; Frequency,	Monetary,	Total Item,	Total Promo Amount,	dan Total Promo Code Used, sisanya memilki korelasi yang rendah. Kemungkin kolom yang memilki relasi tinggi adalah hasil Feature Engineering ðŸ¤¨

## Feature Extraction
"""

selected_cols = num_cols.merge(cat_cols, on = 'Customer ID')

selected_cols.head()

"""### Label Encoding"""

selected_cols = selected_cols.drop('Customer ID', axis=1)

s = (selected_cols.dtypes == 'object')
object_cols = list(s[s].index)

LE=LabelEncoder()
for i in object_cols:
    selected_cols[i]=selected_cols[[i]].apply(LE.fit_transform)

selected_cols['Is Churn'] = selected_cols['Is Churn'].replace({False: 0,
                                                               True: 1})

selected_cols.describe()

"""> Hasil Encode Kolom Kategorikal:
* Gender
 - `0` = Female
 - `1` = Male
* Device Type
 - `0` = Android
 - `1` = iOS
* Payment Method
 - `0` = Credit Card
 - `1` = Debit Card
 - `2` = Gopay
 - `3` = LinkAja
 - `4` = OVO
* Master Category
 - `0` = Accessories
 - `1` = Apparel
 - `2` = Footwear
 - `3` = Free Items
 - `4` = Home
 - `5` = Personal Care
 - `6` = Sporting Goods
* Season
 - `0` = Fall
 - `1` = Spring
 - `2` = Summer
 - `3` = Winter
* Usage
 - `0` = Casual 
 - `1` = Ethnic
 - `2` = Formal
 - `3` = Home
 - `4` = Party
 - `5` = Smart Casual
 - `6` = Sports
 - `7` = Travel
* Main Location
 - `0` = Luar Pulau Jawa
 - `1` = Pulau Jawa
* Is Churn
 - `0` = False
 - `1` = True

### Data Normalization
"""

scaler = MinMaxScaler()
scaler.fit(selected_cols)
scaled_dataset = pd.DataFrame(scaler.transform(selected_cols),columns= selected_cols.columns )

scaled_dataset.describe()

"""### Cek Outlier"""

plt.figure(figsize=(15,7))
sns.boxplot(data=selected_cols, palette='magma')
plt.xticks(rotation=90)

"""### Reducing Dimensionality Dataset"""

pca = PCA(n_components=2)
pca.fit(scaled_dataset)
PCA_ds = pd.DataFrame(pca.transform(scaled_dataset), columns=(["col1","col2"]))
PCA_ds.describe().T

"""### Clustering"""

print('Elbow Method to determine the number of clusters to be formed:')
Elbow_M = KElbowVisualizer(KMeans(), k=10)
Elbow_M.fit(PCA_ds)
Elbow_M.show()

#Initiating the Agglomerative Clustering model 
kmeans = KMeans(n_clusters=4)
# fit model and predict clusters
yhat_kmeans = kmeans.fit_predict(PCA_ds)
#Adding the Clusters feature to the orignal dataframe.
selected_cols["Clusters"] = yhat_kmeans

selected_cols.head()

plt.figure(figsize=(15,7))
sns.countplot(x=selected_cols["Clusters"], palette= 'magma')
plt.title('Distribution Of The Clusters', fontsize=20)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.xlabel('Clusters', fontsize=15)
plt.ylabel('Count', fontsize=15)
plt.show()

selected_cols.columns

column = ['Promo Amount', 'Shipment Fee', 'Total Amount', 'Quantity',
       'Item Price', 'Age', 'Duration', 'Recency', 'Frequency', 'Monetary',
       'Total Item', 'Total Promo Amount', 'Total Promo Code Used',
       'Estimated Delivery', 'Max Shipping', ]
column

sum_customer = selected_cols.groupby('Clusters')['Recency'].count().reset_index()
sum_customer

modus_churn = selected_cols.groupby('Clusters')['Is Churn'].agg(pd.Series.mode).reset_index()
modus_churn

cluster = selected_cols.groupby('Clusters')[column].mean().reset_index()
cluster

cluster['n_cust'] = sum_customer['Recency']
cluster['mode_is_churn'] = modus_churn['Is Churn']
cluster

"""Terdapat 4 grup atau cluster, yaitu:

1. Cluster 0 terdapat 22624 customer, dengan rata-rata recencynya adalah 98 hari, frequency 31, dan monetary 3.784634e+07. Berarti cluster ini memiliki customer yang yang cukup baru-baru saja dalam melakukan pembelian lagi. Rata-rata order 31 kali dan rata-rata total uang yang  dikeluarkan banyak.

2. Cluster 1 terdapat 15615 customer, dengan rata-rata recencynya adalah 98 hari, frequency 32, dan monetary 3.785380e+07. Berarti cluster ini memiliki customer yang cukup baru-baru saja dalam melakukan pembelian lagi. Rata-rata order 32 kali dan rata-rata total uang yang dikeluarkan banyak.

3. Cluster 2 terdapat 7268 customer, dengan rata-rata recencynya adalah 914 hari, frequency 2, dan monetary 2.712550e+06. Berarti cluster ini memiliki customer sudah lama dalam melakukan pembelian lagi. Rata-rata order cuma 2 kali dan rata-rata total uang yang dikeluarkan sedikit.

4. Cluster 3 terdapat 5198 customer, dengan rata-rata recencynya adalah 912 hari, frequency 2, dan monetary 2.952511e+06. Berarti cluster ini memiliki customer sudah lama dalam melakukan pembelian lagi. Rata-rata order cuma 2 kali dan rata-rata total uang yang dikeluarkan sedikit.

Tidak terdapat perbedaan yang  signifakan antara cluster 0,1 dan 2,3. Kedua kluster tersebut memiliki persamaan di RFM nya.

# Modeling

## Preprocessing

### Feature Selection
"""

X = reduced_dataset[['Shipment Fee',
                     'Age', 
                     'Duration', 
                     'Recency',
                     'Frequency',	
                     'Monetary',	
                     'Total Item',
                     'Total Promo Amount',
                     'Total Promo Code Used', 
                     'Max Shipping',
                     'Gender',
                     'Device Type', 
                     'Payment Method',
                     'Main Location']]

y = reduced_dataset['Is Churn']

X

"""### Label Encoding"""

# Encoding
X = pd.get_dummies(X)
X

"""### Data Normalization"""

scaler = MinMaxScaler()
scaler.fit(X)
X = pd.DataFrame(scaler.transform(X),columns= X.columns )

X.describe()

from sklearn.feature_selection import mutual_info_classif

mutual_info = mutual_info_classif(X, y)
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = X.columns
mutual_info.sort_values(ascending=False)

"""## Machine Learning Models

### Split Data
"""

from sklearn.model_selection import train_test_split 

X_train, X_test,\
y_train, y_test = train_test_split(X, y, random_state=42)

print("X_train dataset shape: ", X_train.shape)
print("y_train dataset shape: ", y_train.shape)
print("X_test dataset shape: ", X_test.shape)
print("y_test dataset shape: ", y_test.shape)

"""### Baseline Models"""

#Machine learning Model
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

#Cross Validation -> Untuk menangani data yang tidak balance
from sklearn.model_selection import StratifiedKFold 
from sklearn.model_selection import RepeatedStratifiedKFold

#Evaluation
from sklearn import metrics
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
from sklearn.metrics import make_scorer,accuracy_score,roc_auc_score,precision_score,recall_score,f1_score,log_loss
from sklearn.metrics import confusion_matrix

# Modelling Algorithms
# StratifiedKFold adalah Cross Validation (CV)

kf = StratifiedKFold(n_splits=2,shuffle=True,random_state=42)

## Collect all model in one list
all_model = [DecisionTreeClassifier,
             LogisticRegression,
             KNeighborsClassifier,
             GaussianNB,
             SVC,
             LinearSVC,
             RandomForestClassifier,
             GradientBoostingClassifier,
             ExtraTreesClassifier,
             XGBClassifier]

model_name = ['DecisionTreeClassifier',
              'LogisticRegression',
              'KNeighborsClassifier',
              'GaussianNB',
              'SVC',
              'LinearSVC',
              'RandomForestClassifier',
              'GradientBoostingClassifier',
              'ExtraTreesClassifier',
              'XGBClassifier']
## loop for all model

datatr = []
datasc = []
Recall =[]
Precision =[]
auc =[]

for idx, model_type in enumerate(all_model):
    num = 1
    AccTrain = []
    AccTest = []
    RecallTemp = []
    PrecisionTemp = []
    AucTemp = []
    nfold = 1
    for train_index,test_index in kf.split(X,y): 

        print("----------BEFORE------------")
        print("{} Acc Train: {}, {} of KFold {}".format(model_name[idx], AccTrain, nfold, kf.n_splits))
        print("{} Acc Test: {}, {} of KFold {}".format(model_name[idx], AccTest, nfold, kf.n_splits))
        print("{} Recall: {}, {} of KFold {}".format(model_name[idx], RecallTemp, nfold, kf.n_splits))
        print("{} Precission: {}, {} of KFold {}".format(model_name[idx], PrecisionTemp, nfold, kf.n_splits))
        print("{} AUC: {}, {} of KFold {}".format(model_name[idx], AucTemp, nfold, kf.n_splits))
        print("---------------------------")
        
        X_train, X_test = X.loc[train_index], X.loc[test_index]
        y_train, y_test = y.loc[train_index], y.loc[test_index]
        
        model = model_type()
        model.fit(X_train,y_train)
        y_pred=model.predict(X_test)
        
        AccTrain.append(model.score(X_train , y_train))
        AccTest.append(model.score(X_test , y_test))
        RecallTemp.append(recall_score(y_test,y_pred))
        PrecisionTemp.append(precision_score(y_test,y_pred))
        AucTemp.append(roc_auc_score(y_test, y_pred))
        
        print("----------AFTER------------")
        print("{} Acc Train: {}, {} of KFold {}".format(model_name[idx], AccTrain, nfold, kf.n_splits))
        print("{} Acc Test: {}, {} of KFold {}".format(model_name[idx], AccTest, nfold, kf.n_splits))
        print("{} Recall: {}, {} of KFold {}".format(model_name[idx], RecallTemp, nfold, kf.n_splits))
        print("{} Precission: {}, {} of KFold {}".format(model_name[idx], PrecisionTemp, nfold, kf.n_splits))
        print("{} AUC: {}, {} of KFold {}".format(model_name[idx], AucTemp, nfold, kf.n_splits))
        print("---------------------------")
        
        nfold += 1
    
    print("----------FINAL------------")
    print("{} Acc Train: {}".format(model_name[idx], np.mean(AccTrain)))
    print("{} Acc Test: {}".format(model_name[idx], np.mean(AccTest)))
    print("{} Recall: {}".format(model_name[idx], np.mean(RecallTemp)))
    print("{} Precission: {}".format(model_name[idx], np.mean(PrecisionTemp)))
    print("{} AUC: {}".format(model_name[idx], np.mean(AucTemp)))
    print("---------------------------")
    datatr.append(np.mean(AccTrain))
    datasc.append(np.mean(AccTest))
    Recall.append(np.mean(RecallTemp))
    Precision.append(np.mean(PrecisionTemp))
    auc.append(np.mean(AucTemp))

## compare model each other
data_hasil = pd.DataFrame()
data_hasil['model'] = model_name
data_hasil['Accuracy training'] = datatr
data_hasil['Accuracy test'] = datasc
data_hasil['Precision'] = Precision
data_hasil['Recall']= Recall
data_hasil['AUC'] = auc
data_hasil['gap'] = abs(data_hasil['Accuracy training'] - data_hasil['Accuracy test'])
data_hasil.sort_values(by='Accuracy test',ascending=False)

"""### DecisionTreeClassifier """

model = DecisionTreeClassifier()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

report = classification_report(y_true=y_test, y_pred=model.predict(X_test))
print(report)

confusion = confusion_matrix(y_true=y_test, y_pred=model.predict(X_test))
plt.figure(figsize=(8, 8))
sns.heatmap(confusion,
            xticklabels=["Positive", "Negative"],
            yticklabels=["Positive", "Negative"],
            annot=True, 
            annot_kws={"fontsize": "xx-large"},
            cmap='magma')

plt.show()

predict = pd.DataFrame(y_pred)

real = pd.DataFrame(y_test)

predict.value_counts()

real.value_counts()

"""#### Feature Importance"""

model.fit(X_train, y_train)
fig = go.Figure(go.Bar(
            x=model.feature_importances_,
            y=X_train.columns,
            orientation='h', marker_color='steelblue'))
fig.update_layout(title='<b>Estimating feature importance through the DecisionTreeClassifier model', title_x=0.5, 
                 xaxis_title="Feature importance", yaxis_title='Feature', barmode='stack', yaxis={'categoryorder':'total ascending'})

fig.show()

"""### RandomForestClassifier"""

random_forest = RandomForestClassifier()
random_forest.fit(X_train,y_train)
y_pred=random_forest.predict(X_test)

report_random_forest = classification_report(y_true=y_test, y_pred=random_forest.predict(X_test))
print(report_random_forest)

confusion_random_forest = confusion_matrix(y_true=y_test, y_pred=random_forest.predict(X_test))
plt.figure(figsize=(8, 8))
sns.heatmap(confusion_random_forest,
            xticklabels=["Positive", "Negative"],
            yticklabels=["Positive", "Negative"],
            annot=True, 
            annot_kws={"fontsize": "xx-large"},
            cmap='magma')

plt.show()

"""#### Feature Importance"""

random_forest.fit(X_train, y_train)
fig = go.Figure(go.Bar(
            x=random_forest.feature_importances_,
            y=X_train.columns,
            orientation='h', marker_color='steelblue'))
fig.update_layout(title='<b>Estimating feature importance through the RandomForestClassifier model', title_x=0.5, 
                 xaxis_title="Feature importance", yaxis_title='Feature', barmode='stack', yaxis={'categoryorder':'total ascending'})

fig.show()

"""### GradientBoostingClassifier	"""

gradient_boosting = GradientBoostingClassifier()
gradient_boosting.fit(X_train,y_train)
y_pred=gradient_boosting.predict(X_test)

report_gradient_boosting = classification_report(y_true=y_test, y_pred=gradient_boosting.predict(X_test))
print(report_gradient_boosting)

confusion_gradient_boosting = confusion_matrix(y_true=y_test, y_pred=gradient_boosting.predict(X_test))
plt.figure(figsize=(8, 8))
sns.heatmap(confusion_gradient_boosting,
            xticklabels=["Positive", "Negative"],
            yticklabels=["Positive", "Negative"],
            annot=True, 
            annot_kws={"fontsize": "xx-large"},
            cmap='magma')

plt.show()

"""#### Feature Importance"""

gradient_boosting.fit(X_train, y_train)
fig = go.Figure(go.Bar(
            x=gradient_boosting.feature_importances_,
            y=X_train.columns,
            orientation='h', marker_color='steelblue'))
fig.update_layout(title='<b>Estimating feature importance through the GradientBoostingClassifier model', title_x=0.5, 
                 xaxis_title="Feature importance", yaxis_title='Feature', barmode='stack', yaxis={'categoryorder':'total ascending'})

fig.show()

"""### XGBClassifier"""

XGBClassifier = XGBClassifier()
XGBClassifier.fit(X_train,y_train)
y_pred=XGBClassifier.predict(X_test)

report_XGBClassifier = classification_report(y_true=y_test, y_pred=XGBClassifier.predict(X_test))
print(report_XGBClassifier)

confusion_XGBClassifier = confusion_matrix(y_true=y_test, y_pred=XGBClassifier.predict(X_test))
plt.figure(figsize=(8, 8))
sns.heatmap(confusion_XGBClassifier,
            xticklabels=["Positive", "Negative"],
            yticklabels=["Positive", "Negative"],
            annot=True, 
            annot_kws={"fontsize": "xx-large"},
            cmap='magma')

plt.show()

"""#### Feature Importance"""

XGBClassifier.fit(X_train, y_train)
fig = go.Figure(go.Bar(
            x=XGBClassifier.feature_importances_,
            y=X_train.columns,
            orientation='h', marker_color='steelblue'))
fig.update_layout(title='<b>Estimating feature importance through the XGBClassifier model', title_x=0.5, 
                 xaxis_title="Feature importance", yaxis_title='Feature', barmode='stack', yaxis={'categoryorder':'total ascending'})

fig.show()

"""## Hyperparameter Tuning"""

#Grid Search
from sklearn.model_selection import GridSearchCV

#Train-Test Split
from sklearn.model_selection import train_test_split

"""### DecisionTreeClassifier"""

dt_classifier = DecisionTreeClassifier()

param_dtc = {'max_features': ['auto', 'sqrt', 'log2'],
              'ccp_alpha': [0.1, .01, .001, 0.0001],
              'max_depth' : [5, 10, 20, 40, 80, 160, 320, 640],
              'criterion' :['gini', 'entropy', 'log_loss']
             }

cv = StratifiedKFold(n_splits=2)

dtc = GridSearchCV(estimator=dt_classifier, 
                 param_grid=param_dtc, 
                 cv=cv,
                 verbose=4, 
                 scoring='accuracy') 
dtc.fit(X_train, y_train)

print('Best Score: {}'.format(dtc.best_score_))
print('Best Hyperparameters: {}'.format(dtc.best_params_))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train dataset shape: ", X_train.shape)
print("y_train dataset shape: ", y_train.shape)
print("X_test dataset shape: ", X_test.shape)
print("y_test dataset shape: ", y_test.shape)

# Buat Model Pakai Best Param Dari GridSearchCV
modeltun = DecisionTreeClassifier(**dtc.best_params_)

# Melatih Model Pakai data train kita
model_fit = modeltun.fit(X_train,y_train)

# Cek Peforma model kita di data latih
y_pred_train=model_fit.predict(X_train)
print(classification_report(y_train,y_pred_train))

# Cek Peforma model kita di data test
y_pred_test=model_fit.predict(X_test)
print(classification_report(y_test,y_pred_test))

"""### RandomForestClassifier"""

rf_classifier = RandomForestClassifier()

param_RFC = { 
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 10, 20, 40, 80, 160, 320, 640],
    'criterion' :['gini', 'entropy']
}

cv = StratifiedKFold(n_splits=2)

rfc = GridSearchCV(estimator=rf_classifier, 
                 param_grid=param_RFC,
                 cv = cv, 
                 verbose=4)

rfc.fit(X_train, y_train)

print('Best Score: {}'.format(rfc.best_score_))
print('Best Hyperparameters: {}'.format(rfc.best_params_))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train dataset shape: ", X_train.shape)
print("y_train dataset shape: ", y_train.shape)
print("X_test dataset shape: ", X_test.shape)
print("y_test dataset shape: ", y_test.shape)

# Buat Model Pakai Best Param Dari GridSearchCV
modeltun = RandomForestClassifier(**rfc.best_params_)

# Melatih Model Pakai data train kita
model_fit = modeltun.fit(X_train,y_train)

# Cek Peforma model kita di data latih
y_pred_train=model_fit.predict(X_train)
print(classification_report(y_train,y_pred_train))

# Cek Peforma model kita di data test
y_pred_test=model_fit.predict(X_test)
print(classification_report(y_test,y_pred_test))

confusion = confusion_matrix(y_true=y_test, y_pred=model.predict(X_test))
plt.figure(figsize=(8, 8))
sns.heatmap(confusion,
            xticklabels=["Positive", "Negative"],
            yticklabels=["Positive", "Negative"],
            annot=True, 
            annot_kws={"fontsize": "xx-large"},
            cmap='magma')

plt.show()

""">Confusion Matrix dari Data Testing menunjukan bahwa nilai dari FP (False Positive) dan nilai dari FN (False Negative) seimbang (Symetric) yaitu `0 : 0`, sehingga metrik yang kami gunakan adalah `accuracy` dengan skor `1.0`

### GradientBoostingClassifier
"""

gb_classifier = GradientBoostingClassifier()

param_GBC = {
    "loss":["deviance"],
    "learning_rate": [0.01, 0.05, 0.1, 0.15, 0.2],
    # "min_samples_split": np.linspace(0.1, 0.5, 12),
    # "min_samples_leaf": np.linspace(0.1, 0.5, 12),
    # "max_depth":[3,5,8],
    "max_features":["log2","sqrt"],
    "criterion": ["friedman_mse",  "mae"],
    # "subsample":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],
    "n_estimators":[10]
    }

gbc = StratifiedKFold(n_splits=2)

gbc = GridSearchCV(estimator=gb_classifier, 
                 param_grid=param_GBC,
                 cv = cv, 
                 verbose=4)

gbc.fit(X_train, y_train)

print('Best Score: {}'.format(gbc.best_score_))
print('Best Hyperparameters: {}'.format(gbc.best_params_))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train dataset shape: ", X_train.shape)
print("y_train dataset shape: ", y_train.shape)
print("X_test dataset shape: ", X_test.shape)
print("y_test dataset shape: ", y_test.shape)

modeltun = GradientBoostingClassifier(**gbc.best_params_)

model_fit = modeltun.fit(X_train,y_train)

y_pred_train=model_fit.predict(X_train)
print(classification_report(y_train,y_pred_train))

y_pred_test=model_fit.predict(X_test)
print(classification_report(y_test,y_pred_test))

"""### XGBClassifier"""

xgb_classifier = XGBClassifier()

param_XGBC = {
    'objective':['binary:logistic'],
    'learning_rate': [0.05],
    # 'max_depth': [6],
    # 'min_child_weight': [11],
    'silent': [1],
    'subsample': [0.8],
    # 'colsample_bytree': [0.7],
    'n_estimators': [200, 500],
    # 'missing':[-999],
    # 'seed': [1337]
    }

xgbc = StratifiedKFold(n_splits=2)

xgbc = GridSearchCV(estimator=xgb_classifier, 
                 param_grid=param_XGBC,
                 cv = cv, 
                 verbose=4)

xgbc.fit(X, y)

print('Best Score: {}'.format(xgbc.best_score_))
print('Best Hyperparameters: {}'.format(xgbc.best_params_))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train dataset shape: ", X_train.shape)
print("y_train dataset shape: ", y_train.shape)
print("X_test dataset shape: ", X_test.shape)
print("y_test dataset shape: ", y_test.shape)

modeltun = XGBClassifier(**xgbc.best_params_)

model_fit = modeltun.fit(X_train,y_train)

y_pred_train=model_fit.predict(X_train)
print(classification_report(y_train,y_pred_train))

y_pred_test=model_fit.predict(X_test)
print(classification_report(y_test,y_pred_test))

"""# Recomendation

> Beberapa Rekomendasi dari kami sebagai berikut

* Perusahaan harus meningkatkan lagi nilai dari potongan harga untuk menarik perhatian pelanggan agar melakukan pembelian
* Menambah lebih banyak lagi promo gratis ongkos kirim guna meningkatkan kesetiaan pelanggan terutama yang berada di Pulau Jawa sebagai peminat tertinggi
* Menambah lebih banyak lagi produk-produk yang umumnya disukai perempuan sebagai mayoritas pelanggan
*   Pemberian promo yang lebih besar terhadap penggunaan credit card agar pelanggan tertarik dan dapat mengurangi churn karena pelanggan lebih banyak transaksi menggunakan credit card
*   Pelanggan yang churn merupakan pelanggan tidak kembali lagi ke platform untuk melakukan transaksi. Sehingga, pemberian promo yang menarik atau cashback secara berkala perlu diberikan agar menarik perhatian pelanggan.
*   Model ML yang terpilih untuk memprediksi customer churn adalah model RandomForestClassifier

# END
"""